{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "folder_path = \"\"\n",
    "# img_size = ( 256,256 )\n",
    "\n",
    "path = glob.glob( os.path.join( folder_path, \"*.jpg\" ) )\n",
    "data = np.array([ cv2.imread(path) for path in path ]).astype( 'float32' )\n",
    "\n",
    "# data = np.array([ cv2.resize( data, img_size ) for data in data ])        # 画像サイズを変更する場合\n",
    "data = np.array([ img[ :, :, ::-1] for img in data ])\n",
    "data = [ Image.fromarray( img.astype(np.uint8)).convert('RGB') for img in data ]\n",
    "# data = [ img.transpose((1, 2, 0)) for img in data ]\n",
    "data = np.array(data).transpose((0, 1, 2, 3))\n",
    "print( \"img num:{}, img size:{}\".format( len(data), np.array(data).size ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [0 if (\"Healthy\" in p) else 1 if (\"Damaged\" in p) else 2 for p in path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの確認\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, label, data, transform):\n",
    "        self.transform = transform\n",
    "        self.labelset = label\n",
    "        self.dataset = data\n",
    "\n",
    "        self.datanum = len(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_label = self.labelset[idx]\n",
    "        out_data = self.dataset[idx]\n",
    "\n",
    "        out_label = torch.tensor(out_label,dtype=torch.long)\n",
    "        out_data = self.transform(out_data)\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "batch_size = 100\n",
    "dataset = Mydatasets(label, data, transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size+1])\n",
    "data_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18のプリトレインモデルを読み込み\n",
    "resnet18 = models.resnet18(pretrained = True)\n",
    "backbone = nn.Sequential(*list(resnet18.children())[:-1])   # 最終層を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "        # 全結合層\n",
    "        self.fc = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).reshape(-1, 512)\n",
    "\n",
    "        # 全結合層\n",
    "        fc = self.fc(x)\n",
    "\n",
    "        return fc\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = Model()\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model.to(device), (3,100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数と最適化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習ループ\n",
    "\n",
    "num_epoch = 10\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "test_loss_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss = 0\n",
    "    train_acc  = 0\n",
    "\n",
    "    # 学習時間\n",
    "    start = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for images, labels in data_loader:\n",
    "        # 勾配の初期化(ループの頭でやる必要あり)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 訓練データの準備\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # 順伝搬計算\n",
    "        outputs = model(images)\n",
    "\n",
    "        # 誤差計算\n",
    "        loss = criterion( outputs, labels )\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 学習\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        #予測値算出\n",
    "        predicted = outputs.max(1)[1]\n",
    "\n",
    "        #正解件数算出\n",
    "        train_acc += (predicted == labels).sum()\n",
    "\n",
    "    # # 訓練データに対する損失と精度の計算\n",
    "    avg_train_loss = train_loss / len(data_loader.dataset)\n",
    "    avg_train_acc = train_acc / len(data_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            predicted = outputs.max(1)[1]\n",
    "            test_acc += (predicted == labels).sum()\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    avg_test_acc = test_acc / len(test_loader.dataset)\n",
    "\n",
    "    # 損失と精度の表示\n",
    "    elapsed = time.time() - start\n",
    "    print (f'Epoch [{(epoch+1):2}/{num_epoch}] - {elapsed:0.2f}s - loss: {avg_test_loss:.5f} - acc: {avg_test_acc:.5f}')\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    train_acc_list.append(avg_train_acc)\n",
    "    test_loss_list.append(avg_test_loss)\n",
    "    test_acc_list.append(avg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "label_list = []\n",
    "model.eval()\n",
    "for i,(img, labels) in enumerate(test_loader):\n",
    "    img = img.to(device)\n",
    "    pred = model(img)\n",
    "\n",
    "    label_list.extend(labels.tolist())\n",
    "\n",
    "    pred_list.extend(pred.tolist())\n",
    "\n",
    "pred = np.argmax(pred_list , axis=1 )\n",
    "diff_index = np.arange( len(label_list) )[ label_list != pred]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label:0 = 健康\n",
    "\n",
    "label:1 = 損傷\n",
    "\n",
    "label:2 = 死亡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(diff_index)}/{len(label_list)}\")\n",
    "plt.figure( figsize=(12,16) )\n",
    "\n",
    "for i,diff in enumerate( diff_index[:30] ):\n",
    "    plt.subplot( 6, 5, i+1 )\n",
    "    plt.tight_layout()\n",
    "    plt.title(  \"No.{}, Ture:{}, Pred:{}\".format( diff, label_list[diff], pred[diff] ) )\n",
    "    plt.imshow( data[diff] )\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.savefig(\"ResNet50-5.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def line_notify(text):\n",
    "\n",
    "    line_notify_token = ''\n",
    "    line_notify_api = ''\n",
    "\n",
    "    message = ('\\n{0}'.format(text))\n",
    "\n",
    "    payload = {'message': message}\n",
    "    headers = {'Authorization': 'Bearer ' + line_notify_token}\n",
    "    requests.post(line_notify_api, data=payload, headers=headers)\n",
    "\n",
    "line_notify('python')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
